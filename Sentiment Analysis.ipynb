{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating and Visualizing Polarity Independence Scores \n",
    "\n",
    "This notebook allows for the easy implementation of the sentiment analysis-based portion of the media capture measurement. This method requires a dataset of the following format:\n",
    "\n",
    "|           | outlet   | date   | text         |\n",
    "| --------- | -------- | ------ | ------------ |\n",
    "| article 1 | outlet 1 | date 1 | article text |\n",
    "| article 2 | ...      | ...    | ...          |\n",
    "\n",
    "The outlet column should identify\n",
    "- the names of the specific nominally independent outlets under investigation\n",
    "- a group of outlets identified as opposition outlets outside of the reach of the regime\n",
    "- a group of outlets identified as belonging to the regime through state ownership, family, or party ties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ec4d48",
   "metadata": {},
   "source": [
    "## Sentiment Classification\n",
    "\n",
    "This first half of the notebook is concerned with classifying the sentiment of the sentences mentioning the regime leader.\n",
    "\n",
    "To begin, we need to download the needed libraries. Uncomment the two top lines if you are running this notebook for the first time to install the sentiment classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92d738f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pysentimiento\n",
    "# !pip install transformers\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import re\n",
    "import pysentimiento\n",
    "import pickle\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3aff0df",
   "metadata": {},
   "source": [
    "With this code chunk, specify the location of the dataset containing the article texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55470f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset with preprocessed text\n",
    "data_location = 'Data/dataset_token_ready.csv'\n",
    "\n",
    "df = pd.read_csv(data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82f3ad8",
   "metadata": {},
   "source": [
    "The following code chunk prepares the dataset for sentiment analysis. Most importantly, you need to specify how sentences containing mentions of the regime leader should be identified. For example, in our article, we wanted to extract mentions of the president regime leader. For this, we extracted sentences containing the following terms (both with and without capital letters:) regime leader, Nuestro Presidente, Presidente de Nicaragua, Comandante Daniel, and Daniel y Rosario.\n",
    "\n",
    "In code, we used the following string:\n",
    "\n",
    "```r\n",
    "regex = r'([Oo]rtega)|([Nn]uestro [Pp]residente)|[Pp]residente de [Nn]icaragua|([Cc]omandante [Dd]aniel)|[Dd]aniel y [Rr]osario'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb09b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define regex to search for mentions of regime leader\n",
    "regex = r'([Ee]xample1)|([Tt]wo-word [Ee]xample)'\n",
    "\n",
    "# subset articles that contain mentions of regime leader\n",
    "df = df.loc[df['text'].str.contains(regex, na = False)].reset_index(drop = True)\n",
    "\n",
    "# split string of texts into list of sentences\n",
    "df[\"sentences\"] = df.text.apply(lambda x: re.split(\"[.!?]\", x))\n",
    "\n",
    "# explode rows so that each row contains one sentence\n",
    "df = df.explode(\"sentences\", ignore_index = True)\n",
    "\n",
    "# subset sentences that contain mentions of regime leader\n",
    "df = df.loc[df[\"sentences\"].str.contains(regex)].reset_index(drop = True)\n",
    "\n",
    "# if sentence longer than 200 words, keep only 90 word window around first mention of regime leader\n",
    "def trim_sentence(sentence, regex):\n",
    "    words = sentence.split()\n",
    "    if len(words) > 200:\n",
    "        match = re.search(regex, sentence)\n",
    "        if match:\n",
    "            start_index = match.start()\n",
    "            start_word_index = len(sentence[:start_index].split())\n",
    "            window_start = max(0, start_word_index - 90)\n",
    "            window_end = min(len(words), start_word_index + 90)\n",
    "            return ' '.join(words[window_start:window_end])\n",
    "    return sentence\n",
    "\n",
    "df[\"sentences\"] = df[\"sentences\"].apply(lambda x: trim_sentence(x, regex))\n",
    "\n",
    "# drop text column\n",
    "df.drop(\"text\", axis = 1, inplace = True)\n",
    "\n",
    "sentences = df[\"sentences\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac2dd42",
   "metadata": {},
   "source": [
    "The following code chunks specify which sentiment classifier should be used. This model can be run with any classifier, but we provide two examples here. The first code chunk sets pysentimiento, the classifier we used in our article, which is well-suited for Spanish. The second code chunk instead sets a distilbert-based classifier trained by Lik Xun Yuan, described here on hugging face: https://huggingface.co/lxyuan/distilbert-base-multilingual-cased-sentiments-student. This model is multilingual and works for\n",
    "\n",
    "- English\n",
    "- Arabic\n",
    "- German\n",
    "- Spanish\n",
    "- French\n",
    "- Japanese\n",
    "- Chinese\n",
    "- Indonesian\n",
    "- Hindi\n",
    "- Italian\n",
    "- Malay\n",
    "- Portuguese\n",
    "\n",
    "In terms of classification results, pysentimiento tends to classify more sentences as neutral relative to negative, and especially positive.\n",
    "\n",
    "The distilbert-based classifier tends to view very few sentences as neutral and instead finds more positive and negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5189bf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Execute this code chunk to set pysentimiento as the sentiment analysis tool ####################\n",
    "\n",
    "analyzer = pysentimiento.create_analyzer(task=\"sentiment\", lang=\"es\")\n",
    "\n",
    "\n",
    "# define function to analyze sentiment of sentence\n",
    "def analyze_sentences(sentences):\n",
    "    results = []\n",
    "    for sentence in tqdm(sentences):\n",
    "        sentiment = analyzer.predict(sentence).output\n",
    "        results.append(sentiment)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a086d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Execute this code chunk to set distilbert as the sentiment analysis tool ####################\n",
    "\n",
    "analyser_distil = pipeline(\n",
    "    model=\"lxyuan/distilbert-base-multilingual-cased-sentiments-student\", \n",
    "    return_all_scores=False\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# define function to analyze sentiment of sentence\n",
    "def analyze_sentences(sentences):\n",
    "    results = []\n",
    "    for sentence in tqdm(sentences):\n",
    "        sentiment = analyser_distil(sentence)[0][\"label\"]\n",
    "        results.append(sentiment)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb2ce2f",
   "metadata": {},
   "source": [
    "This code chunk executes the sentiment analysis and saves the result in a new column called \"sentiment\". Be aware that this will take some time. On our machine, pysentimiento runs at about 20 sentences per second, and the distilbert classifier at around 30. This translates to a computing time of between 55 and 83 minutes for 100,000 sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c71171d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = analyze_sentences(sentences)\n",
    "df[\"sentiment\"] = results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4058ee4c",
   "metadata": {},
   "source": [
    "Finally, this code saves the dataset with the classified sentiment in the chosen location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad57103",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_location = 'chosen/location/filename.pkl'\n",
    "\n",
    "with open(save_location, \"wb\") as f:\n",
    "    pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20128b22",
   "metadata": {},
   "source": [
    "## Computing the Polarity Score and the Polarity Independence Score\n",
    "\n",
    "The following code is used to compute the polarity score for each chosen outlet/group of outlets in the specified time intervals.\n",
    "\n",
    "The polarity score shows the average sentiment of mentions of the regime leader per time bracket. A score of 1 means that all mentions in the chosen time bracket are positive, -1 would mean that they are exclusively negative, while 0 signifies a balance between positive and negative mentions.\n",
    "\n",
    "The polarity independence score shows the relative difference in polarity between nominally independent outlets and the regime-owned outlets. If an outlet scores 1, it reports as negatively on the regime as the opposition outlets (which are assumed to be independent of the regime). If it scores 0, then it reports in line with the regime preference.\n",
    "\n",
    "The first code chunk loads the dataset with the classified sentiment. Edit the data_location object to point to the location of the dataset created with the first half of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8edc856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset with preprocessed text\n",
    "data_location = 'location/dataset_with_sentiment.pkl'\n",
    "\n",
    "with open(data_location, \"rb\") as f:\n",
    "    df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127b0576",
   "metadata": {},
   "source": [
    "The following chunk creates a variety of time aggregations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1bd5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform date to datetime format\n",
    "df['date'] = pd.to_datetime(df['date'], format='mixed', errors='coerce')\n",
    "\n",
    "# create different time periods\n",
    "df['year'] = df['year'].dt.strftime('%Y')\n",
    "\n",
    "df[\"quarter\"] = df.date.dt.to_period('Q')\n",
    "df['quarter'] = df['quarter'].dt.strftime('%Y-%m')\n",
    "\n",
    "# Create semiannual periods\n",
    "def get_semiannual_period(date):\n",
    "    year = date.year\n",
    "    if date.month <= 6:\n",
    "        return f\"{year}Q1\"\n",
    "    else:\n",
    "        return f\"{year}Q3\"\n",
    "\n",
    "df['semiannual'] = df['date'].apply(get_semiannual_period)\n",
    "df['semiannual'] = df['semiannual'].dt.strftime('%Y-%m')\n",
    "\n",
    "df['year_month'] = df['date'].dt.strftime('%Y-%m')\n",
    "\n",
    "df['week'] = df['date'].dt.strftime('%Y-%U')\n",
    "\n",
    "df['day'] = df['date'].dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf5213a",
   "metadata": {},
   "source": [
    "Now we choose the time periods for which we want to compute the scores. These can be\n",
    "- year\n",
    "- semiannual\n",
    "- quarter\n",
    "- year_month\n",
    "- week\n",
    "- day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d0d772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this variable to desired aggregation level\n",
    "agg_level = \"year_month\"\n",
    "\n",
    "\n",
    "df[\"date\"] = df[agg_level]\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da31bb9e",
   "metadata": {},
   "source": [
    "The following code chunk computes the polarity score and saves the resulting dataframe. You will need to edit the save location to the desired directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca2528a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create aggregated overview of sentiment per outlet and year-quarter\n",
    "df_agg = (df.groupby([\"outlet\", \"date\"])[\"sentiment\"]\n",
    "          .value_counts(normalize=True)\n",
    "          .rename(\"proportion\")\n",
    "          .reset_index())\n",
    "\n",
    "# Create polarity variable\n",
    "df_agg.loc[df_agg[\"sentiment\"] == \"neutral\", \"polarity\"] = 0\n",
    "df_agg.loc[df_agg[\"sentiment\"] == \"positive\", \"polarity\"] = df_agg[\"proportion\"]\n",
    "df_agg.loc[df_agg[\"sentiment\"] == \"negative\", \"polarity\"] = df_agg[\"proportion\"] * -1\n",
    "\n",
    "# Create polarity aggregated data\n",
    "df_pol = df_agg.groupby([\"outlet\", \"date\"]).agg({\"polarity\": np.sum}).reset_index()\n",
    "\n",
    "# save data\n",
    "save_location = 'chosen/location/filename.csv'\n",
    "df_pol.to_csv(save_location, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fbec23",
   "metadata": {},
   "source": [
    "The following code chunk computes the independence score. Unlike the polarity score, the independence score is only computed only for the nominally independent outlets operating inside the country relative to the regime outlets and the opposition outlets. In the following code chunk you will need to enter the names of the outlets/outlet groups in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989f7c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlet_list = [\"nominally independent outlet1\", \"nominally independent outlet2\"]\n",
    "regime_column = \"name_regime_group\"\n",
    "opposition_column = \"name_opposition_group\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622b6c28",
   "metadata": {},
   "source": [
    "Finally, this code chunk computes the polarity independence score and saves the resulting dataframe as a csv file. The resulting file can be used for statistical analysis or to visualise trends in independence of the different independent outlets over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee20dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate independence score\n",
    "def calculate_independence_score(df, outlets, regime_column, opposition_column):\n",
    "    \"\"\"\n",
    "    This function calculates the independence score of a media outlet based on the difference in sentiment between\n",
    "    the regime and opposition.\n",
    "    Regime and opposition columns can either identify single outlets, but were intended to be used as aggregated\n",
    "    columns that represent the sentiment in all regime and opposition outlets.\n",
    "    \"\"\"\n",
    "    for outlet in outlets:\n",
    "        score = (((abs(df[outlet] - df[regime_column]) - abs(df[outlet] - df[opposition_column])) /\n",
    "                abs(df[opposition_column] - df[regime_column])) + 1) / 2\n",
    "        df[outlet] = score\n",
    "\n",
    "# Create table to see how many mentions per outlet per year\n",
    "df_can = pd.pivot(df_pol, index=\"date\", columns=\"outlet\", values=\"polarity\")\n",
    "\n",
    "# calculate independence score\n",
    "calculate_independence_score(df_can, outlet_list, regime_column, opposition_column)\n",
    "\n",
    "# Melt back to long format\n",
    "df_ind = pd.melt(df_can.reset_index(), id_vars=['date'],\n",
    "                 value_vars=outlet_list,\n",
    "                 value_name=\"independence score\")\n",
    "\n",
    "# save data\n",
    "save_location = 'chosen/location/filename.csv'\n",
    "df_ind.to_csv(save_location, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
